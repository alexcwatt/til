# Foundation Models

Traditionally, machine learning models have been trained for specific use cases. Foundation models offer an alternate approach: Train a model on a massive and diverse data set so that it is general purpose and can be adapted to many different tasks.

Here are examples of current families of foundation models:

* Large language models (LLMs) like GPT, Claude, and Gemini have been trained on massive amounts of text on the Internet and are applicable to many use cases. They can be used to build chat bots, to summarize text, to manage tool calls, to write and debug code, and more.
* Multimodal models like CLIP are able to map images and text into a shared embedding space. They can be used to understand images. Some multimodal models, like DALLÂ·E and Stable Diffusion, are even able to generate images based on text.

While LLMs get a lot of attention, there are many foundation models being developed for specific domains. For example, OpenFold is used to predict the 3D structures of proteins and other biomolecules; this is useful for research and development of drugs.
